{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoML\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/kuennethgroup/colab_tutorials/blob/main/lecture8/autoML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->\n",
        "\n",
        "1. Prepare data\n",
        "1. We train a AutoML model using this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>property</th>\n",
              "      <th>value</th>\n",
              "      <th>fingerprint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[*]C[*]</td>\n",
              "      <td>Xc</td>\n",
              "      <td>47.80</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[*]CC([*])C</td>\n",
              "      <td>Xc</td>\n",
              "      <td>44.47</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[*]CC([*])CC</td>\n",
              "      <td>Xc</td>\n",
              "      <td>34.04</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[*]CC([*])CCC</td>\n",
              "      <td>Xc</td>\n",
              "      <td>20.01</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[*]CC([*])CC(C)C</td>\n",
              "      <td>Xc</td>\n",
              "      <td>21.64</td>\n",
              "      <td>[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>[*]C([*])(F)F</td>\n",
              "      <td>Xc</td>\n",
              "      <td>31.84</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>[*]C/C=C\\C[*]</td>\n",
              "      <td>Xc</td>\n",
              "      <td>25.58</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>[*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(N=Cc3ccc(OC(=O)CCC[Si]([*])(C)C)cc3)cc2)cc1</td>\n",
              "      <td>Xc</td>\n",
              "      <td>29.05</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>[*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(Cc3ccc(N=Cc4ccc(OC(=O)CCC[Si]([*])(C)C)cc4)cc3)cc2)cc1</td>\n",
              "      <td>Xc</td>\n",
              "      <td>21.74</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>[*]CCN(CCCCCCOc1ccc(C=Cc2ccc([N+](=O)[O-])cc2)cc1)CCOC(=O)NCCCCCCNC(=O)O[*]</td>\n",
              "      <td>Xc</td>\n",
              "      <td>7.55</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows \u00d7 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                           smiles  \\\n",
              "0                                                                                         [*]C[*]   \n",
              "1                                                                                     [*]CC([*])C   \n",
              "2                                                                                    [*]CC([*])CC   \n",
              "3                                                                                   [*]CC([*])CCC   \n",
              "4                                                                                [*]CC([*])CC(C)C   \n",
              "..                                                                                            ...   \n",
              "427                                                                                 [*]C([*])(F)F   \n",
              "428                                                                                 [*]C/C=C\\C[*]   \n",
              "429             [*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(N=Cc3ccc(OC(=O)CCC[Si]([*])(C)C)cc3)cc2)cc1   \n",
              "430  [*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(Cc3ccc(N=Cc4ccc(OC(=O)CCC[Si]([*])(C)C)cc4)cc3)cc2)cc1   \n",
              "431                   [*]CCN(CCCCCCOc1ccc(C=Cc2ccc([N+](=O)[O-])cc2)cc1)CCOC(=O)NCCCCCCNC(=O)O[*]   \n",
              "\n",
              "    property  value  \\\n",
              "0         Xc  47.80   \n",
              "1         Xc  44.47   \n",
              "2         Xc  34.04   \n",
              "3         Xc  20.01   \n",
              "4         Xc  21.64   \n",
              "..       ...    ...   \n",
              "427       Xc  31.84   \n",
              "428       Xc  25.58   \n",
              "429       Xc  29.05   \n",
              "430       Xc  21.74   \n",
              "431       Xc   7.55   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                            fingerprint  \n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "1     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "2     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "3     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "4     [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "..                                                                                                                                                                                                                                                                                                                  ...  \n",
              "427   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "428   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "429   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "430   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "431  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "\n",
              "[432 rows x 4 columns]"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_ = pd.read_json(\n",
        "    \"https://raw.githubusercontent.com/kuennethgroup/colab_tutorials/main/lecture2/data/polymers_tend_to_crystalize.json\"\n",
        ")\n",
        "# ... and easy-peasy\n",
        "df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-learn autogluon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>fingerprint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47.80</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44.47</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34.04</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.01</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.64</td>\n",
              "      <td>[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>31.84</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>25.58</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>29.05</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>21.74</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>7.55</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows \u00d7 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     value  \\\n",
              "0    47.80   \n",
              "1    44.47   \n",
              "2    34.04   \n",
              "3    20.01   \n",
              "4    21.64   \n",
              "..     ...   \n",
              "427  31.84   \n",
              "428  25.58   \n",
              "429  29.05   \n",
              "430  21.74   \n",
              "431   7.55   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                            fingerprint  \n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "1     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "2     [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "3     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "4     [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "..                                                                                                                                                                                                                                                                                                                  ...  \n",
              "427   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "428   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "429   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "430   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "431  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]  \n",
              "\n",
              "[432 rows x 2 columns]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ = df_.drop(columns=[\"property\", \"smiles\"])\n",
        "df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47.80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44.47</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34.04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.01</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.64</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>31.84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>25.58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>29.05</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>21.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>7.55</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows \u00d7 2049 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     value  0  1  2  3  4  5  6  7  8  ...  2038  2039  2040  2041  2042  \\\n",
              "0    47.80  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "1    44.47  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "2    34.04  1  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "3    20.01  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "4    21.64  0  2  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "..     ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
              "427  31.84  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "428  25.58  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "429  29.05  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "430  21.74  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "431   7.55  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "\n",
              "     2043  2044  2045  2046  2047  \n",
              "0       0     0     0     0     0  \n",
              "1       0     0     0     0     0  \n",
              "2       0     0     0     0     0  \n",
              "3       0     0     0     0     0  \n",
              "4       0     0     0     0     0  \n",
              "..    ...   ...   ...   ...   ...  \n",
              "427     0     0     0     0     0  \n",
              "428     0     0     0     0     0  \n",
              "429     0     0     0     0     0  \n",
              "430     0     0     0     0     0  \n",
              "431     0     0     0     0     1  \n",
              "\n",
              "[432 rows x 2049 columns]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "df = df_[\"value\"].to_frame()\n",
        "df = pd.concat((df, pd.DataFrame(np.vstack(df_[\"fingerprint\"]))), axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(432, 345, 87)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.20, random_state=42)\n",
        "len(df), len(df_train), len(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>57.620000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>28.370000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>21.610000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>79.990000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>0.370000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>29.770000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>21.110000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>8.900000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>12.490000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>31.755734</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>345 rows \u00d7 2049 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         value  0  1  2  3  4  5  6  7  8  ...  2038  2039  2040  2041  2042  \\\n",
              "132  57.620000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "231  28.370000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "31   21.610000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "84   79.990000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "296   0.370000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "..         ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
              "71   29.770000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "106  21.110000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "270   8.900000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "348  12.490000  0  0  0  0  0  0  0  0  0  ...     0     0     0     1     0   \n",
              "102  31.755734  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "\n",
              "     2043  2044  2045  2046  2047  \n",
              "132     0     0     0     0     0  \n",
              "231     0     0     0     0     0  \n",
              "31      0     0     0     0     0  \n",
              "84      0     0     0     0     0  \n",
              "296     0     0     0     0     0  \n",
              "..    ...   ...   ...   ...   ...  \n",
              "71      0     0     0     0     0  \n",
              "106     0     0     0     0     0  \n",
              "270     0     0     0     0     0  \n",
              "348     0     0     0     0     0  \n",
              "102     0     0     0     0     0  \n",
              "\n",
              "[345 rows x 2049 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>8.380000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>72.530000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>53.050000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>16.730000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>38.130000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>62.840000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>88.390000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>12.637895</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>37.020000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>22.980000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87 rows \u00d7 2049 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         value  0  1  2  3  4  5  6  7  8  ...  2038  2039  2040  2041  2042  \\\n",
              "424   8.380000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "75   72.530000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "180  53.050000  0  0  0  0  0  0  0  0  1  ...     0     0     0     0     0   \n",
              "30   16.730000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "392  38.130000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "..         ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
              "57   62.840000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "124  88.390000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "24   12.637895  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "17   37.020000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "66   22.980000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "\n",
              "     2043  2044  2045  2046  2047  \n",
              "424     0     0     0     0     0  \n",
              "75      0     0     0     0     0  \n",
              "180     0     0     0     0     0  \n",
              "30      0     0     0     0     0  \n",
              "392     0     0     0     0     0  \n",
              "..    ...   ...   ...   ...   ...  \n",
              "57      0     0     0     0     0  \n",
              "124     0     0     0     0     0  \n",
              "24      0     0     0     0     0  \n",
              "17      0     0     0     0     0  \n",
              "66      0     0     0     0     0  \n",
              "\n",
              "[87 rows x 2049 columns]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(df_train)\n",
        "display(train_data)\n",
        "\n",
        "test_data = TabularDataset(df_test)\n",
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20231212_083438\"\n",
            "Presets specified: ['high_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
            "Sub-fit(s) time limit is: 120 seconds.\n",
            "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20231212_083438/ds_sub_fit/sub_fit_ho.\n",
            "Running the sub-fit in a ray process to avoid memory leakage.\n",
            "Presets specified: ['high_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
            "Sub-fit(s) time limit is: 120 seconds.\n",
            "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20231212_083438/ds_sub_fit/sub_fit_ho.\n",
            "Running the sub-fit in a ray process to avoid memory leakage.\n",
            "Spend 43 seconds for the sub-fit(s) during dynamic stacking.\n",
            "Time left for full fit of AutoGluon: 77 seconds.\n",
            "Starting full fit now with num_stack_levels 0.\n",
            "Beginning AutoGluon training ... Time limit = 77s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231212_083438\"\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.0.0\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #92-Ubuntu SMP Mon Aug 14 09:30:42 UTC 2023\n",
            "CPU Count:          192\n",
            "Memory Avail:       938.16 GB / 1007.45 GB (93.1%)\n",
            "Disk Space Avail:   4508.04 GB / 7096.34 GB (63.5%)\n",
            "===================================================\n",
            "Train Data Rows:    345\n",
            "Train Data Columns: 2048\n",
            "Label Column:       value\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    960678.56 MB\n",
            "\tTrain Data (Original)  Memory Usage: 5.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 843 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 780): ['3', '6', '10', '15', '16', '18', '19', '21', '23', '28', '35', '38', '40', '42', '46', '51', '56', '60', '62', '63', '64', '65', '66', '71', '72', '77', '78', '82', '85', '86', '88', '89', '91', '92', '95', '96', '101', '103', '104', '107', '108', '111', '112', '113', '119', '120', '121', '123', '124', '127', '128', '129', '130', '134', '136', '137', '138', '141', '149', '150', '151', '153', '154', '155', '159', '160', '161', '168', '169', '172', '177', '178', '179', '181', '182', '186', '189', '194', '198', '200', '205', '207', '211', '215', '216', '218', '223', '224', '229', '238', '241', '242', '244', '250', '256', '258', '259', '263', '266', '268', '272', '274', '277', '278', '280', '284', '286', '290', '291', '296', '297', '298', '299', '300', '309', '313', '321', '327', '328', '329', '331', '332', '334', '335', '337', '338', '344', '345', '346', '349', '351', '356', '360', '362', '365', '369', '370', '371', '380', '382', '384', '388', '390', '394', '395', '396', '397', '400', '402', '403', '404', '405', '409', '410', '412', '413', '414', '415', '416', '417', '418', '419', '421', '425', '426', '427', '431', '433', '434', '435', '439', '441', '442', '446', '447', '448', '450', '452', '454', '455', '456', '457', '458', '459', '462', '464', '465', '468', '470', '474', '475', '479', '491', '492', '493', '496', '500', '501', '505', '506', '508', '513', '514', '517', '520', '523', '524', '528', '529', '531', '532', '533', '535', '536', '542', '543', '546', '550', '551', '552', '554', '556', '557', '560', '565', '566', '571', '575', '576', '586', '593', '595', '596', '601', '602', '610', '611', '613', '614', '616', '623', '624', '627', '630', '631', '634', '635', '641', '646', '647', '651', '661', '665', '669', '672', '678', '688', '689', '690', '696', '697', '700', '702', '704', '705', '708', '709', '712', '716', '717', '720', '722', '724', '727', '730', '731', '732', '735', '737', '738', '740', '747', '748', '752', '754', '756', '757', '758', '762', '765', '768', '771', '773', '777', '778', '780', '782', '786', '788', '791', '792', '793', '795', '800', '805', '808', '809', '813', '815', '816', '817', '818', '819', '821', '833', '837', '840', '842', '844', '847', '849', '852', '855', '861', '869', '872', '873', '874', '876', '884', '885', '887', '889', '890', '894', '899', '902', '903', '911', '913', '917', '919', '920', '922', '927', '928', '930', '938', '939', '943', '944', '947', '949', '950', '952', '955', '958', '962', '963', '965', '970', '971', '973', '974', '976', '977', '978', '980', '982', '985', '988', '989', '990', '992', '997', '1000', '1001', '1002', '1005', '1011', '1016', '1018', '1021', '1022', '1025', '1026', '1029', '1030', '1033', '1035', '1037', '1040', '1041', '1046', '1047', '1053', '1058', '1059', '1065', '1072', '1073', '1075', '1078', '1079', '1086', '1092', '1093', '1094', '1097', '1099', '1101', '1102', '1104', '1105', '1106', '1112', '1115', '1121', '1122', '1124', '1129', '1130', '1131', '1133', '1134', '1136', '1137', '1139', '1141', '1148', '1149', '1150', '1153', '1154', '1159', '1166', '1169', '1170', '1171', '1177', '1181', '1183', '1186', '1190', '1192', '1194', '1198', '1202', '1204', '1206', '1207', '1211', '1212', '1214', '1216', '1217', '1218', '1220', '1226', '1230', '1231', '1233', '1236', '1237', '1241', '1242', '1246', '1247', '1250', '1253', '1256', '1257', '1258', '1259', '1260', '1261', '1264', '1265', '1268', '1270', '1272', '1273', '1275', '1279', '1281', '1284', '1285', '1288', '1291', '1293', '1294', '1295', '1298', '1300', '1302', '1304', '1305', '1307', '1311', '1312', '1315', '1316', '1320', '1329', '1334', '1336', '1337', '1338', '1339', '1340', '1342', '1346', '1351', '1354', '1355', '1358', '1359', '1361', '1367', '1368', '1369', '1371', '1373', '1376', '1377', '1381', '1387', '1388', '1389', '1390', '1395', '1396', '1397', '1401', '1402', '1403', '1404', '1405', '1406', '1407', '1408', '1410', '1412', '1414', '1418', '1423', '1426', '1428', '1429', '1431', '1433', '1434', '1437', '1438', '1443', '1447', '1448', '1449', '1451', '1458', '1461', '1465', '1468', '1469', '1474', '1475', '1477', '1478', '1483', '1484', '1486', '1488', '1491', '1496', '1497', '1498', '1500', '1504', '1505', '1509', '1510', '1511', '1519', '1526', '1528', '1529', '1532', '1535', '1537', '1538', '1541', '1543', '1547', '1548', '1549', '1550', '1552', '1554', '1556', '1558', '1560', '1561', '1562', '1563', '1566', '1567', '1568', '1570', '1572', '1574', '1577', '1578', '1583', '1584', '1591', '1593', '1595', '1596', '1598', '1600', '1601', '1613', '1615', '1620', '1621', '1623', '1626', '1628', '1629', '1631', '1635', '1636', '1637', '1638', '1640', '1642', '1644', '1648', '1651', '1657', '1662', '1663', '1664', '1667', '1670', '1675', '1677', '1678', '1680', '1681', '1682', '1684', '1687', '1689', '1690', '1696', '1698', '1701', '1702', '1704', '1706', '1707', '1709', '1711', '1716', '1723', '1724', '1730', '1732', '1736', '1739', '1742', '1744', '1748', '1752', '1756', '1759', '1761', '1763', '1765', '1767', '1771', '1776', '1780', '1783', '1784', '1787', '1789', '1790', '1792', '1794', '1797', '1801', '1808', '1813', '1815', '1817', '1818', '1820', '1828', '1832', '1833', '1835', '1836', '1837', '1838', '1841', '1852', '1859', '1861', '1864', '1866', '1869', '1871', '1872', '1874', '1875', '1879', '1880', '1881', '1890', '1891', '1892', '1893', '1897', '1902', '1903', '1906', '1907', '1909', '1914', '1918', '1921', '1927', '1931', '1933', '1936', '1938', '1943', '1944', '1946', '1949', '1953', '1954', '1957', '1966', '1967', '1969', '1972', '1973', '1974', '1976', '1977', '1980', '1981', '1986', '1987', '1994', '1998', '1999', '2005', '2007', '2010', '2011', '2012', '2014', '2015', '2017', '2018', '2024', '2025', '2028', '2029', '2030', '2037', '2038', '2040', '2042', '2044', '2046']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 386): ['47', '61', '67', '75', '146', '157', '163', '185', '199', '201', '204', '208', '234', '246', '254', '255', '257', '260', '262', '265', '271', '285', '304', '305', '306', '315', '317', '318', '320', '324', '341', '353', '364', '372', '377', '381', '406', '428', '445', '451', '472', '485', '490', '498', '499', '502', '509', '512', '516', '525', '527', '537', '538', '549', '558', '561', '563', '567', '568', '582', '585', '599', '600', '603', '629', '633', '638', '643', '644', '648', '649', '653', '654', '655', '657', '658', '659', '663', '666', '675', '681', '701', '707', '723', '726', '729', '741', '750', '764', '767', '772', '774', '776', '779', '796', '797', '798', '802', '810', '811', '812', '822', '825', '828', '838', '839', '845', '853', '860', '863', '871', '877', '880', '895', '897', '904', '906', '914', '916', '925', '936', '941', '946', '960', '966', '968', '979', '984', '986', '993', '1004', '1020', '1027', '1032', '1042', '1048', '1049', '1050', '1051', '1060', '1061', '1062', '1067', '1068', '1069', '1080', '1082', '1084', '1085', '1089', '1091', '1095', '1098', '1100', '1108', '1109', '1111', '1118', '1123', '1125', '1126', '1127', '1140', '1142', '1155', '1165', '1167', '1168', '1172', '1173', '1175', '1184', '1187', '1189', '1193', '1195', '1197', '1203', '1208', '1210', '1215', '1219', '1222', '1223', '1225', '1228', '1235', '1239', '1245', '1248', '1266', '1271', '1278', '1289', '1296', '1306', '1310', '1322', '1324', '1330', '1331', '1332', '1333', '1335', '1347', '1348', '1353', '1360', '1362', '1363', '1372', '1374', '1378', '1379', '1383', '1393', '1400', '1409', '1411', '1413', '1416', '1421', '1424', '1439', '1445', '1455', '1457', '1463', '1464', '1467', '1470', '1472', '1479', '1481', '1487', '1489', '1490', '1492', '1503', '1506', '1508', '1512', '1514', '1515', '1516', '1517', '1521', '1523', '1527', '1530', '1531', '1534', '1540', '1542', '1545', '1553', '1557', '1559', '1569', '1573', '1576', '1580', '1585', '1597', '1604', '1605', '1606', '1607', '1608', '1609', '1611', '1614', '1618', '1619', '1624', '1625', '1627', '1630', '1633', '1634', '1639', '1646', '1650', '1653', '1654', '1656', '1659', '1672', '1674', '1685', '1692', '1708', '1710', '1712', '1713', '1720', '1721', '1725', '1726', '1733', '1735', '1738', '1741', '1745', '1746', '1751', '1760', '1762', '1764', '1773', '1778', '1779', '1781', '1782', '1786', '1791', '1793', '1796', '1803', '1805', '1807', '1812', '1814', '1819', '1827', '1829', '1834', '1844', '1845', '1848', '1850', '1858', '1860', '1862', '1863', '1865', '1867', '1868', '1870', '1877', '1883', '1894', '1898', '1899', '1900', '1901', '1904', '1905', '1908', '1912', '1913', '1919', '1922', '1923', '1925', '1929', '1932', '1935', '1937', '1939', '1941', '1950', '1955', '1960', '1961', '1965', '1968', '1971', '1975', '1983', '1993', '1997', '2001', '2003', '2006', '2008', '2013', '2021', '2022', '2026', '2031', '2032', '2034', '2039', '2043', '2045']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('int', []) : 386 | ['47', '61', '67', '75', '146', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', []) : 882 | ['0', '1', '2', '4', '5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('int', [])       : 409 | ['1', '2', '8', '9', '11', ...]\n",
            "\t\t('int', ['bool']) : 473 | ['0', '4', '5', '7', '12', ...]\n",
            "\t2.5s = Fit runtime\n",
            "\t882 features in original data used to generate 882 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.23 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 2.56s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 108 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 74.44s of the 74.43s of remaining time.\n",
            "\t-20.2691\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 74.15s of the 74.14s of remaining time.\n",
            "\t-19.8639\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 73.84s of the 73.83s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
            "\t-18.1373\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.93s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 71.42s of the 71.41s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
            "\t-18.6287\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.97s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 68.96s of the 68.95s of remaining time.\n",
            "\t-19.0034\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.61s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 68.13s of the 68.12s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\t-18.4688\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.68s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 62.87s of the 62.86s of remaining time.\n",
            "\t-19.0719\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.55s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 62.13s of the 62.12s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-19.4314\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.82s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 57.77s of the 57.76s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.04%)\n",
            "\t-18.5863\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.12s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 55.15s of the 55.14s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-17.7685\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.61s\t = Training   runtime\n",
            "\t1.09s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 47.88s of the 47.87s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
            "\t-18.3859\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.47s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 43.91s of the 43.9s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\t-18.2642\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.24s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 40.11s of the 40.1s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-18.9713\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.51s\t = Training   runtime\n",
            "\t1.07s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 34.0s of the 33.99s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\t-18.7921\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.67s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 30.73s of the 30.72s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-20.1717\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.52s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 25.71s of the 25.71s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.14%)\n",
            "\t-18.1139\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.06s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 19.13s of the 19.12s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
            "\t-19.3045\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.96s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 13.66s of the 13.65s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-17.4717\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.9s\t = Training   runtime\n",
            "\t1.12s\t = Validation runtime\n",
            "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 4.14s of the 4.13s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.28%)\n",
            "\t-19.022\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.85s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 74.44s of the -0.61s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r22_BAG_L1': 0.406, 'NeuralNetTorch_r79_BAG_L1': 0.219, 'LightGBMLarge_BAG_L1': 0.146, 'LightGBMXT_BAG_L1': 0.125, 'NeuralNetTorch_BAG_L1': 0.104}\n",
            "\t-16.775\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.26s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 77.9s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.2s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\t1.05s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t0.89s\t = Training   runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.61s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\t0.8s\t = Training   runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.55s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
            "\tStopping at the best epoch learned earlier - 12.\n",
            "\t0.53s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\t0.31s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\t1.74s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
            "\t1.27s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r177_BAG_L1_FULL ...\n",
            "\t0.69s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1_FULL ...\n",
            "\t1.55s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r131_BAG_L1_FULL ...\n",
            "\t2.1s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1_FULL ...\n",
            "\tStopping at the best epoch learned earlier - 14.\n",
            "\t0.73s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_r9_BAG_L1_FULL ...\n",
            "\t1.45s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_r96_BAG_L1_FULL ...\n",
            "\t4.66s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1_FULL ...\n",
            "\t3.11s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_r33_BAG_L1_FULL ...\n",
            "\t1.21s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r22_BAG_L1': 0.406, 'NeuralNetTorch_r79_BAG_L1': 0.219, 'LightGBMLarge_BAG_L1': 0.146, 'LightGBMXT_BAG_L1': 0.125, 'NeuralNetTorch_BAG_L1': 0.104}\n",
            "\t0.26s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 23.66s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231212_083438\")\n"
          ]
        }
      ],
      "source": [
        "predictor = TabularPredictor(\n",
        "    label=\"value\",\n",
        "    problem_type=\"regression\",\n",
        ").fit(train_data, time_limit=120, presets=\"high_quality\")\n",
        "\n",
        "# presets at https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'root_mean_squared_error': -17.44446201808955,\n",
              " 'mean_squared_error': -304.30925510056886,\n",
              " 'mean_absolute_error': -13.167327296947201,\n",
              " 'r2': 0.5004311424614001,\n",
              " 'pearsonr': 0.7203412850171623,\n",
              " 'median_absolute_error': -10.238773880004883}"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = predictor.evaluate(test_data, silent=True)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'pred')"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "y_pred = predictor.predict(test_data.drop(columns=[\"value\"]))\n",
        "y_pred\n",
        "\n",
        "ax.plot(y_pred, test_data[\"value\"], \"o\")\n",
        "ax.plot([0, 100], [0, 100], \"k--\")\n",
        "ax.text(0.1, 0.9, f\"R2 = {metrics['r2']:.3f}\", transform=ax.transAxes)\n",
        "ax.set_title(\"Testdata set\")\n",
        "ax.set_ylabel(\"truth\")\n",
        "ax.set_xlabel(\"pred\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_test</th>\n",
              "      <th>score_val</th>\n",
              "      <th>eval_metric</th>\n",
              "      <th>pred_time_test</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_test_marginal</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WeightedEnsemble_L2_FULL</td>\n",
              "      <td>-17.444462</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.455911</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.990445</td>\n",
              "      <td>0.002475</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.263717</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NeuralNetTorch_r22_BAG_L1_FULL</td>\n",
              "      <td>-17.843945</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.133598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.107995</td>\n",
              "      <td>0.133598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.107995</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NeuralNetTorch_BAG_L1_FULL</td>\n",
              "      <td>-18.089460</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.127097</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.742981</td>\n",
              "      <td>0.127097</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.742981</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n",
              "      <td>-18.159527</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.016834</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.525315</td>\n",
              "      <td>0.016834</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.525315</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost_r33_BAG_L1_FULL</td>\n",
              "      <td>-18.543248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.040659</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.209735</td>\n",
              "      <td>0.040659</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.209735</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LightGBM_r131_BAG_L1_FULL</td>\n",
              "      <td>-18.559354</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.035077</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.099811</td>\n",
              "      <td>0.035077</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.099811</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CatBoost_r9_BAG_L1_FULL</td>\n",
              "      <td>-18.633597</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.029004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.453585</td>\n",
              "      <td>0.029004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.453585</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LightGBM_BAG_L1_FULL</td>\n",
              "      <td>-18.646686</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.036046</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.887521</td>\n",
              "      <td>0.036046</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.887521</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1_FULL</td>\n",
              "      <td>-18.786852</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.052241</td>\n",
              "      <td>0.093935</td>\n",
              "      <td>0.549826</td>\n",
              "      <td>0.052241</td>\n",
              "      <td>0.093935</td>\n",
              "      <td>0.549826</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1</td>\n",
              "      <td>-18.786852</td>\n",
              "      <td>-19.071912</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.054021</td>\n",
              "      <td>0.093935</td>\n",
              "      <td>0.549826</td>\n",
              "      <td>0.054021</td>\n",
              "      <td>0.093935</td>\n",
              "      <td>0.549826</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CatBoost_BAG_L1_FULL</td>\n",
              "      <td>-19.016991</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.035643</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800215</td>\n",
              "      <td>0.035643</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.800215</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>RandomForestMSE_BAG_L1</td>\n",
              "      <td>-19.139208</td>\n",
              "      <td>-19.003386</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.053381</td>\n",
              "      <td>0.104738</td>\n",
              "      <td>0.606289</td>\n",
              "      <td>0.053381</td>\n",
              "      <td>0.104738</td>\n",
              "      <td>0.606289</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>RandomForestMSE_BAG_L1_FULL</td>\n",
              "      <td>-19.139208</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.057899</td>\n",
              "      <td>0.104738</td>\n",
              "      <td>0.606289</td>\n",
              "      <td>0.057899</td>\n",
              "      <td>0.104738</td>\n",
              "      <td>0.606289</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>LightGBMXT_BAG_L1_FULL</td>\n",
              "      <td>-19.174944</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.033286</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.053391</td>\n",
              "      <td>0.033286</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.053391</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>CatBoost_r177_BAG_L1_FULL</td>\n",
              "      <td>-19.501452</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.032599</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.694157</td>\n",
              "      <td>0.032599</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.694157</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>KNeighborsDist_BAG_L1</td>\n",
              "      <td>-19.674458</td>\n",
              "      <td>-19.863875</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.171385</td>\n",
              "      <td>0.202472</td>\n",
              "      <td>0.034209</td>\n",
              "      <td>0.171385</td>\n",
              "      <td>0.202472</td>\n",
              "      <td>0.034209</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>KNeighborsDist_BAG_L1_FULL</td>\n",
              "      <td>-19.674458</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.172357</td>\n",
              "      <td>0.202472</td>\n",
              "      <td>0.034209</td>\n",
              "      <td>0.172357</td>\n",
              "      <td>0.202472</td>\n",
              "      <td>0.034209</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NeuralNetTorch_r79_BAG_L1_FULL</td>\n",
              "      <td>-19.727728</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.127046</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.550254</td>\n",
              "      <td>0.127046</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.550254</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>KNeighborsUnif_BAG_L1</td>\n",
              "      <td>-19.928502</td>\n",
              "      <td>-20.269091</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.096334</td>\n",
              "      <td>0.197723</td>\n",
              "      <td>0.033672</td>\n",
              "      <td>0.096334</td>\n",
              "      <td>0.197723</td>\n",
              "      <td>0.033672</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>KNeighborsUnif_BAG_L1_FULL</td>\n",
              "      <td>-19.928502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.114624</td>\n",
              "      <td>0.197723</td>\n",
              "      <td>0.033672</td>\n",
              "      <td>0.114624</td>\n",
              "      <td>0.197723</td>\n",
              "      <td>0.033672</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>XGBoost_BAG_L1_FULL</td>\n",
              "      <td>-19.937957</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.051480</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.309421</td>\n",
              "      <td>0.051480</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.309421</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>NeuralNetFastAI_r191_BAG_L1_FULL</td>\n",
              "      <td>-19.958208</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.018183</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.726977</td>\n",
              "      <td>0.018183</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.726977</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LightGBM_r96_BAG_L1_FULL</td>\n",
              "      <td>-20.106489</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.042087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.662620</td>\n",
              "      <td>0.042087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.662620</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>LightGBMLarge_BAG_L1_FULL</td>\n",
              "      <td>-20.553978</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.032410</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.272108</td>\n",
              "      <td>0.032410</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.272108</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-16.774995</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.348101</td>\n",
              "      <td>21.682585</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.263717</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>NeuralNetTorch_r22_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-17.471653</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.115835</td>\n",
              "      <td>7.900915</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.115835</td>\n",
              "      <td>7.900915</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>NeuralNetTorch_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-17.768508</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.089529</td>\n",
              "      <td>5.613467</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.089529</td>\n",
              "      <td>5.613467</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>CatBoost_r9_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.113920</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.270543</td>\n",
              "      <td>5.063142</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.270543</td>\n",
              "      <td>5.063142</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>LightGBMXT_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.137322</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.025455</td>\n",
              "      <td>0.927441</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.025455</td>\n",
              "      <td>0.927441</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>CatBoost_r177_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.264246</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.273136</td>\n",
              "      <td>2.239962</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.273136</td>\n",
              "      <td>2.239962</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>LightGBMLarge_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.385876</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.047575</td>\n",
              "      <td>2.467956</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.047575</td>\n",
              "      <td>2.467956</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>CatBoost_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.468801</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.256543</td>\n",
              "      <td>3.678973</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.256543</td>\n",
              "      <td>3.678973</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>XGBoost_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.586261</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048755</td>\n",
              "      <td>1.116966</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048755</td>\n",
              "      <td>1.116966</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>LightGBM_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.628670</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.024664</td>\n",
              "      <td>0.972797</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.024664</td>\n",
              "      <td>0.972797</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>LightGBM_r131_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.792103</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.033506</td>\n",
              "      <td>1.669060</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.033506</td>\n",
              "      <td>1.669060</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>NeuralNetTorch_r79_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.971250</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.069467</td>\n",
              "      <td>4.509089</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.069467</td>\n",
              "      <td>4.509089</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>XGBoost_r33_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.021983</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.064104</td>\n",
              "      <td>2.845771</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.064104</td>\n",
              "      <td>2.845771</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>LightGBM_r96_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.304496</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056061</td>\n",
              "      <td>3.964329</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056061</td>\n",
              "      <td>3.964329</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>NeuralNetFastAI_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.431368</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.062268</td>\n",
              "      <td>2.816817</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.062268</td>\n",
              "      <td>2.816817</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>NeuralNetFastAI_r191_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-20.171720</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.079844</td>\n",
              "      <td>3.515969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.079844</td>\n",
              "      <td>3.515969</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               model  score_test  score_val  \\\n",
              "0           WeightedEnsemble_L2_FULL  -17.444462        NaN   \n",
              "1     NeuralNetTorch_r22_BAG_L1_FULL  -17.843945        NaN   \n",
              "2         NeuralNetTorch_BAG_L1_FULL  -18.089460        NaN   \n",
              "3        NeuralNetFastAI_BAG_L1_FULL  -18.159527        NaN   \n",
              "4            XGBoost_r33_BAG_L1_FULL  -18.543248        NaN   \n",
              "5          LightGBM_r131_BAG_L1_FULL  -18.559354        NaN   \n",
              "6            CatBoost_r9_BAG_L1_FULL  -18.633597        NaN   \n",
              "7               LightGBM_BAG_L1_FULL  -18.646686        NaN   \n",
              "8          ExtraTreesMSE_BAG_L1_FULL  -18.786852        NaN   \n",
              "9               ExtraTreesMSE_BAG_L1  -18.786852 -19.071912   \n",
              "10              CatBoost_BAG_L1_FULL  -19.016991        NaN   \n",
              "11            RandomForestMSE_BAG_L1  -19.139208 -19.003386   \n",
              "12       RandomForestMSE_BAG_L1_FULL  -19.139208        NaN   \n",
              "13            LightGBMXT_BAG_L1_FULL  -19.174944        NaN   \n",
              "14         CatBoost_r177_BAG_L1_FULL  -19.501452        NaN   \n",
              "15             KNeighborsDist_BAG_L1  -19.674458 -19.863875   \n",
              "16        KNeighborsDist_BAG_L1_FULL  -19.674458        NaN   \n",
              "17    NeuralNetTorch_r79_BAG_L1_FULL  -19.727728        NaN   \n",
              "18             KNeighborsUnif_BAG_L1  -19.928502 -20.269091   \n",
              "19        KNeighborsUnif_BAG_L1_FULL  -19.928502        NaN   \n",
              "20               XGBoost_BAG_L1_FULL  -19.937957        NaN   \n",
              "21  NeuralNetFastAI_r191_BAG_L1_FULL  -19.958208        NaN   \n",
              "22          LightGBM_r96_BAG_L1_FULL  -20.106489        NaN   \n",
              "23         LightGBMLarge_BAG_L1_FULL  -20.553978        NaN   \n",
              "24               WeightedEnsemble_L2         NaN -16.774995   \n",
              "25         NeuralNetTorch_r22_BAG_L1         NaN -17.471653   \n",
              "26             NeuralNetTorch_BAG_L1         NaN -17.768508   \n",
              "27                CatBoost_r9_BAG_L1         NaN -18.113920   \n",
              "28                 LightGBMXT_BAG_L1         NaN -18.137322   \n",
              "29              CatBoost_r177_BAG_L1         NaN -18.264246   \n",
              "30              LightGBMLarge_BAG_L1         NaN -18.385876   \n",
              "31                   CatBoost_BAG_L1         NaN -18.468801   \n",
              "32                    XGBoost_BAG_L1         NaN -18.586261   \n",
              "33                   LightGBM_BAG_L1         NaN -18.628670   \n",
              "34              LightGBM_r131_BAG_L1         NaN -18.792103   \n",
              "35         NeuralNetTorch_r79_BAG_L1         NaN -18.971250   \n",
              "36                XGBoost_r33_BAG_L1         NaN -19.021983   \n",
              "37               LightGBM_r96_BAG_L1         NaN -19.304496   \n",
              "38            NeuralNetFastAI_BAG_L1         NaN -19.431368   \n",
              "39       NeuralNetFastAI_r191_BAG_L1         NaN -20.171720   \n",
              "\n",
              "                eval_metric  pred_time_test  pred_time_val   fit_time  \\\n",
              "0   root_mean_squared_error        0.455911            NaN   8.990445   \n",
              "1   root_mean_squared_error        0.133598            NaN   3.107995   \n",
              "2   root_mean_squared_error        0.127097            NaN   1.742981   \n",
              "3   root_mean_squared_error        0.016834            NaN   0.525315   \n",
              "4   root_mean_squared_error        0.040659            NaN   1.209735   \n",
              "5   root_mean_squared_error        0.035077            NaN   2.099811   \n",
              "6   root_mean_squared_error        0.029004            NaN   1.453585   \n",
              "7   root_mean_squared_error        0.036046            NaN   0.887521   \n",
              "8   root_mean_squared_error        0.052241       0.093935   0.549826   \n",
              "9   root_mean_squared_error        0.054021       0.093935   0.549826   \n",
              "10  root_mean_squared_error        0.035643            NaN   0.800215   \n",
              "11  root_mean_squared_error        0.053381       0.104738   0.606289   \n",
              "12  root_mean_squared_error        0.057899       0.104738   0.606289   \n",
              "13  root_mean_squared_error        0.033286            NaN   1.053391   \n",
              "14  root_mean_squared_error        0.032599            NaN   0.694157   \n",
              "15  root_mean_squared_error        0.171385       0.202472   0.034209   \n",
              "16  root_mean_squared_error        0.172357       0.202472   0.034209   \n",
              "17  root_mean_squared_error        0.127046            NaN   1.550254   \n",
              "18  root_mean_squared_error        0.096334       0.197723   0.033672   \n",
              "19  root_mean_squared_error        0.114624       0.197723   0.033672   \n",
              "20  root_mean_squared_error        0.051480            NaN   0.309421   \n",
              "21  root_mean_squared_error        0.018183            NaN   0.726977   \n",
              "22  root_mean_squared_error        0.042087            NaN   4.662620   \n",
              "23  root_mean_squared_error        0.032410            NaN   1.272108   \n",
              "24  root_mean_squared_error             NaN       3.348101  21.682585   \n",
              "25  root_mean_squared_error             NaN       1.115835   7.900915   \n",
              "26  root_mean_squared_error             NaN       1.089529   5.613467   \n",
              "27  root_mean_squared_error             NaN       0.270543   5.063142   \n",
              "28  root_mean_squared_error             NaN       0.025455   0.927441   \n",
              "29  root_mean_squared_error             NaN       0.273136   2.239962   \n",
              "30  root_mean_squared_error             NaN       0.047575   2.467956   \n",
              "31  root_mean_squared_error             NaN       0.256543   3.678973   \n",
              "32  root_mean_squared_error             NaN       0.048755   1.116966   \n",
              "33  root_mean_squared_error             NaN       0.024664   0.972797   \n",
              "34  root_mean_squared_error             NaN       0.033506   1.669060   \n",
              "35  root_mean_squared_error             NaN       1.069467   4.509089   \n",
              "36  root_mean_squared_error             NaN       0.064104   2.845771   \n",
              "37  root_mean_squared_error             NaN       0.056061   3.964329   \n",
              "38  root_mean_squared_error             NaN       0.062268   2.816817   \n",
              "39  root_mean_squared_error             NaN       0.079844   3.515969   \n",
              "\n",
              "    pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  \\\n",
              "0                  0.002475                     NaN           0.263717   \n",
              "1                  0.133598                     NaN           3.107995   \n",
              "2                  0.127097                     NaN           1.742981   \n",
              "3                  0.016834                     NaN           0.525315   \n",
              "4                  0.040659                     NaN           1.209735   \n",
              "5                  0.035077                     NaN           2.099811   \n",
              "6                  0.029004                     NaN           1.453585   \n",
              "7                  0.036046                     NaN           0.887521   \n",
              "8                  0.052241                0.093935           0.549826   \n",
              "9                  0.054021                0.093935           0.549826   \n",
              "10                 0.035643                     NaN           0.800215   \n",
              "11                 0.053381                0.104738           0.606289   \n",
              "12                 0.057899                0.104738           0.606289   \n",
              "13                 0.033286                     NaN           1.053391   \n",
              "14                 0.032599                     NaN           0.694157   \n",
              "15                 0.171385                0.202472           0.034209   \n",
              "16                 0.172357                0.202472           0.034209   \n",
              "17                 0.127046                     NaN           1.550254   \n",
              "18                 0.096334                0.197723           0.033672   \n",
              "19                 0.114624                0.197723           0.033672   \n",
              "20                 0.051480                     NaN           0.309421   \n",
              "21                 0.018183                     NaN           0.726977   \n",
              "22                 0.042087                     NaN           4.662620   \n",
              "23                 0.032410                     NaN           1.272108   \n",
              "24                      NaN                0.000239           0.263717   \n",
              "25                      NaN                1.115835           7.900915   \n",
              "26                      NaN                1.089529           5.613467   \n",
              "27                      NaN                0.270543           5.063142   \n",
              "28                      NaN                0.025455           0.927441   \n",
              "29                      NaN                0.273136           2.239962   \n",
              "30                      NaN                0.047575           2.467956   \n",
              "31                      NaN                0.256543           3.678973   \n",
              "32                      NaN                0.048755           1.116966   \n",
              "33                      NaN                0.024664           0.972797   \n",
              "34                      NaN                0.033506           1.669060   \n",
              "35                      NaN                1.069467           4.509089   \n",
              "36                      NaN                0.064104           2.845771   \n",
              "37                      NaN                0.056061           3.964329   \n",
              "38                      NaN                0.062268           2.816817   \n",
              "39                      NaN                0.079844           3.515969   \n",
              "\n",
              "    stack_level  can_infer  fit_order  \n",
              "0             2       True         40  \n",
              "1             1       True         38  \n",
              "2             1       True         30  \n",
              "3             1       True         28  \n",
              "4             1       True         39  \n",
              "5             1       True         34  \n",
              "6             1       True         36  \n",
              "7             1       True         24  \n",
              "8             1       True         27  \n",
              "9             1       True          7  \n",
              "10            1       True         26  \n",
              "11            1       True          5  \n",
              "12            1       True         25  \n",
              "13            1       True         23  \n",
              "14            1       True         32  \n",
              "15            1       True          2  \n",
              "16            1       True         22  \n",
              "17            1       True         33  \n",
              "18            1       True          1  \n",
              "19            1       True         21  \n",
              "20            1       True         29  \n",
              "21            1       True         35  \n",
              "22            1       True         37  \n",
              "23            1       True         31  \n",
              "24            2      False         20  \n",
              "25            1      False         18  \n",
              "26            1      False         10  \n",
              "27            1      False         16  \n",
              "28            1      False          3  \n",
              "29            1      False         12  \n",
              "30            1      False         11  \n",
              "31            1      False          6  \n",
              "32            1      False          9  \n",
              "33            1      False          4  \n",
              "34            1      False         14  \n",
              "35            1      False         13  \n",
              "36            1      False         19  \n",
              "37            1      False         17  \n",
              "38            1      False          8  \n",
              "39            1      False         15  "
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.leaderboard(test_data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}